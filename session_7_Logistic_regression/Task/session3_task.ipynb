{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNqiLdrjuamz"
      },
      "source": [
        "Your task today is challenging. You will need to partially follow the Machine Learning <strong><span style=\"color:skyblue\">life cycle</span></strong> in order to answer some questions related to <strong><span style=\"color:salmon\">Stroke prediction</span></strong>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc6MSjpEuanB"
      },
      "source": [
        "## Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53pfOztOuanE"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1c_KhDzF4hD229MnFW5QY3TtErn-61BUh\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVV131cNuanG"
      },
      "source": [
        "# <span style=\"color:skyblue\">Business objective</span>\n",
        "> Before jumpping to code and trying to fill in the TODOs meanless üòØ\n",
        "<span style=\"color:red\">You should understand What you are trying to solve and what questions you need to answer!</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1g0bMH8uanI"
      },
      "source": [
        "| Problem type | medical üíäü•º |\n",
        "| --- | --- |\n",
        "| Data type  | tabular üíª(structured) |\n",
        "| Topic | Stroke prediction üß† |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRa9ThOuanJ"
      },
      "source": [
        "According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkJ2q4c5uanL"
      },
      "source": [
        "## TODO üéØ\n",
        "> read and search about this problem\n",
        "- [ÿ≥ŸÉÿ™ÿ© ÿØŸÖÿßÿ∫Ÿäÿ©](!https://ar.wikipedia.org/wiki/%D8%B3%D9%83%D8%AA%D8%A9_%D8%AF%D9%85%D8%A7%D8%BA%D9%8A%D8%A9)\n",
        "- [Is It a Stroke or a Heart Attack?](!https://www.healthline.com/health/stroke-vs-heart-attack#causes)\n",
        "- [Heart Disease and Stroke](!https://www.webmd.com/heart-disease/stroke)\n",
        "- [What is stroke?](!https://www.heartandstroke.ca/stroke/what-is-stroke)\n",
        "- [Stroke after a heart attack: What‚Äôs the risk?](!https://www.health.harvard.edu/heart-health/stroke-after-a-heart-attack-whats-the-risk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA8qTZu4uanN"
      },
      "source": [
        "> feel free to search from other resources if you want\n",
        "\n",
        ">\"The formulation of the problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill.\"  <span style=\"color:red\"> - Albert Einstein</span>\n",
        "\n",
        "> \"It is not the answer that enlightens, but the question.\"<span style=\"color:red\"> - Eugene Ionesco  </span>\n",
        "\n",
        ">\"The art and science of asking questions is the source of all knowledge.\"  <span style=\"color:red\">- Thomas Berger</span>\n",
        "\n",
        "><span style=\"color:red\">\"asking the right question is the half way to the solution\"</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-sE0EODuanP"
      },
      "source": [
        "> now list the information you gathered in this cell\n",
        "- 80% Heart strokes are preventable (example)\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "> What is your questions that you need the data to Answerü§î?\n",
        "- Does age has impact on strokes? and How is this parameter distributed?\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-_Qnp7uanR"
      },
      "source": [
        "# <span style=\"color:skyblue\">Gathering Data</span>\n",
        "> I did it for you ü•∞ so go and import the data file using `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCWmuk_JuanS"
      },
      "outputs": [],
      "source": [
        "# importing pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6MgqRvpuanW"
      },
      "outputs": [],
      "source": [
        "# TODO: import the dataset\n",
        "df = None\n",
        "# Displaying the head of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zbeqXRvuanX",
        "outputId": "3b7d4541-cbbf-40e8-de09-358523cceb2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5110, 12)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the shape of the dataset\n",
        "# 5110 rows\n",
        "# 12 cols\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evA9L1qvuana"
      },
      "outputs": [],
      "source": [
        "# let's see more info about the dataset\n",
        "# TODO: note the messing values you need to deal with them\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zofqAew1uanb"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1g0cWV9sEt_7BYu3OPcVqb9dNAF5CA6f3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFlv86tbuane"
      },
      "source": [
        ">You'll notice that we have various types of variables, and we need to handle each type differently in order to get them ready for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPVAkGCUuanf"
      },
      "outputs": [],
      "source": [
        "# you can restate your questions a little bit if you think your question can't be answered with the data or you can add more questions inspired by the dataset features\n",
        "# stroke is our target\n",
        "# let's describe our data statistically\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaOrnXDouanf"
      },
      "outputs": [],
      "source": [
        "# TODO: describe the categorical features only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4QYOTIauang"
      },
      "outputs": [],
      "source": [
        "# the columns  of our dataset\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8U-uBHGuanh"
      },
      "source": [
        "# TODO üéØ\n",
        "> list the features we have\n",
        "- <span style=\"color:skyblue\">Categorical features</span>\n",
        "  - stroke (example)\n",
        "  -\n",
        "- <span style=\"color:skyblue\">Numerical features</span>\n",
        "  - <span style=\"color:salmon\">Discrete</span>\n",
        "    - id (example)\n",
        "    -\n",
        "    -\n",
        "  - <span style=\"color:salmon\">Continous</span>\n",
        "    - age\n",
        "    -\n",
        "    -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tm4clTmuani"
      },
      "source": [
        "# <span style=\"color:skyblue\">Data Preperation</span>\n",
        "> okay in this phase we should deal with the problems in the dataset\n",
        "ü¶æ problems we should solve üíª\n",
        "- Missing values ‚úÖ\n",
        "- Encoding categorical features that are not numbers ‚ùå we would delay it after EDA\n",
        "- Scalling the data ‚ùå we would delay it after EDA\n",
        "- Feature extraction\n",
        "> EDA is  Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxIv5WJwuani"
      },
      "outputs": [],
      "source": [
        "# TODO : find the missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBvp-rniuanj"
      },
      "outputs": [],
      "source": [
        "# TODO : fill the missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQRkL_V4uank"
      },
      "source": [
        "## Feature extraction\n",
        ">It involves selecting and transforming raw data into a set of meaningful features that can effectively represent the underlying patterns and characteristics of the data.\n",
        "\n",
        "Dimensionality reduction: Feature extraction can help in reduceing the dimensionality of the data by selecting or creating a subset of relevant features. This is particularly valuable when working with high-dimensional datasets, as it reduces computational complexity, improves model performance, and mitigates the risk of overfitting.\n",
        "\n",
        "Improved model performance: By extracting informative and discriminative features, the resulting feature set can enhance the performance of machine learning models. Relevant features provide more accurate representations of the data, enabling models to capture and generalize patterns effectively.\n",
        "\n",
        "Noise reduction: Feature extraction can help filter out irrelevant or noisy features, improving the signal-to-noise ratio in the data. Removing noisy features can lead to more robust and accurate models by reducing the impact of irrelevant information.\n",
        "\n",
        "Improved interpretability: <span style=\"color:red\">Extracting meaningful features </span> can enhance the interpretability of the data and models. By transforming raw data into more understandable and intuitive representations, it becomes easier to interpret and explain the relationships and patterns discovered by the models.\n",
        "\n",
        "Domain-specific knowledge incorporation: Feature extraction enables the incorporation of domain-specific knowledge and expertise into the analysis process. By selecting or designing features that align with domain knowledge, the resulting models can capture relevant aspects of the data that are important for decision-making or problem-solving in specific domains.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCe39ASUuank"
      },
      "source": [
        "<span style=\"color:red\" >Read this function docs</span>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.cut.html\n",
        "```\n",
        "pandas.cut\n",
        "pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise', ordered=True)\n",
        "```\n",
        "\n",
        "Bin values into discrete intervals.\n",
        "\n",
        "Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IZuP1hsuanl"
      },
      "outputs": [],
      "source": [
        "# feature extraction\n",
        "# bmi into categories\n",
        "# 'Underweight', 'Ideal', 'Overweight', 'Obesity'\n",
        "# example üåü\n",
        "df['bmi_cat'] = pd.cut(df['bmi'], bins = [0, 19, 25,30,10000], labels = ['Underweight', 'Ideal', 'Overweight', 'Obesity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYlPylcruanl"
      },
      "outputs": [],
      "source": [
        "# TODO : age into categories\n",
        "# 'Children', 'Teens', 'Adults','Mid Adults','Elderly'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwZ3TE0Guanm"
      },
      "outputs": [],
      "source": [
        "# TODO : glucose into categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QUtPEa0uann"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ZfF5D0uann"
      },
      "source": [
        "# <span style=\"color:skyblue\">Exploratory Data Analysis üé®</span>\n",
        "> time to show your artüé≠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t23LWmm-uano"
      },
      "outputs": [],
      "source": [
        "# importing visualisation library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlRpYL2Puano"
      },
      "outputs": [],
      "source": [
        "# Count unique values in the 'stroke' column\n",
        "stroke_counts = df['stroke'].value_counts()\n",
        "\n",
        "# Create a pie chart\n",
        "plt.pie(stroke_counts, labels=['No', 'Yes'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'salmon'], explode = (0.1, 0))\n",
        "\n",
        "\n",
        "# Display the chart\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oalJFcpTuanp"
      },
      "outputs": [],
      "source": [
        "# TODO : visualise the age categories and the target\n",
        "# Answering question related to what age categories is more likely to have a stroke?ü§î\n",
        "# you are free to do it with your style and with the way you see it suitable I want to see your solution and ideas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqEby3Etuanp"
      },
      "outputs": [],
      "source": [
        "# TODO : visualise the gender and the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UA3Cs_euanq"
      },
      "outputs": [],
      "source": [
        "# TODO : visualise the hypertension and the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fVzy5fyuanr"
      },
      "outputs": [],
      "source": [
        "# TODO : visualise ever_married with the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrEmxnvIuanr"
      },
      "outputs": [],
      "source": [
        "# TODO : visualise glucose_cat with the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4G25tGIuanr"
      },
      "outputs": [],
      "source": [
        "# TODO : Residence_type and the target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJQw2nc9uans"
      },
      "outputs": [],
      "source": [
        "# TODO : smoking_status and the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fztzr5kvuan8"
      },
      "outputs": [],
      "source": [
        "# TODO : build a Correlation Matrix and visualise it\n",
        "# explain how the correlation matrix prove you point and the visualstion you made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AohBgK7uan9"
      },
      "outputs": [],
      "source": [
        "# more visualisation more appreciated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TiPuCYyuan-"
      },
      "source": [
        "# TODO üéØ\n",
        "> now you should be able to answer most of your stated questions\n",
        "> ANSWER them useing a prove from the data and the visuals you made\n",
        "-\n",
        "-\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m1aVJgLuan_"
      },
      "source": [
        "# <span style=\"color:skyblue\"> MORE Data Preperation</span>\n",
        "- Encoding the Strings (categorical features) into numbers so the model can deal with it\n",
        "    - Why the model can't deal with strings ?\n",
        "    - The model do some math operations matrix multplications needed and this need numbers\n",
        "    - Numbers have a meaning to the machien Language no\n",
        "- scaling the data (Normalization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_km7Rpzkuan_"
      },
      "source": [
        "## Encoding\n",
        ">encoding is the process of converting categorical (non-numeric) data into a numerical format that can be used as input for machine learning algorithms.\n",
        "\n",
        "1. **One-Hot Encoding**: This is the most common method. It creates a binary column for each category in the original feature. For example, if you have a \"Color\" feature with categories \"Red,\" \"Blue,\" and \"Green,\" one-hot encoding would create three binary columns: \"IsRed,\" \"IsBlue,\" and \"IsGreen.\" Each row would have a 1 in the column corresponding to its color and 0s in the other columns.\n",
        "    \n",
        "    Pros:\n",
        "    \n",
        "    - Preserves all information.\n",
        "    - Works well with most algorithms.\n",
        "    \n",
        "    Cons:\n",
        "    \n",
        "    - Can create a large number of new features if the original categorical feature has many unique categories.\n",
        "    - May lead to multicollinearity (correlation) among the new features.\n",
        "\n",
        "![image.png](https://drive.google.com/uc?export=view&id=1ei2wyw73ypsh_XBk2KcSlykubCHXg0Kx)\n",
        "\n",
        "\n",
        "2. **Label Encoding**: In label encoding, each category is assigned a unique integer. For example, \"Red\" might be encoded as 0, \"Blue\" as 1, and \"Green\" as 2.\n",
        "    \n",
        "    Pros:\n",
        "    \n",
        "    - Simple and efficient.\n",
        "    - Reduces dimensionality compared to one-hot encoding.\n",
        "    \n",
        "    Cons:\n",
        "    \n",
        "    - Can introduce ordinal relationships that don't exist in the data (e.g., implying that Green is \"greater\" than Red, which may not be the case).\n",
        "    \n",
        "![image-2.png](https://drive.usercontent.google.com/download?id=135YvqDbva6aqAKyugOOJdWfNTqYYGwcW&export=view&authuser=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRIYb720uaoA"
      },
      "outputs": [],
      "source": [
        "# checking datatypes we have\n",
        "# int and float are numerical by nature so we foucs on encoding the object and category types\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaSGcJdIuaoB"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O', 'category'])\n",
        "# to see only the columns names\n",
        "# df.select_dtypes(include=['O', 'category']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTquJJ73uaoC"
      },
      "outputs": [],
      "source": [
        "# we are running this code to know the categories in each column (feature)\n",
        "objcols = df.select_dtypes(include=['O', 'category']).columns\n",
        "for col in objcols:\n",
        "    print('='*40)\n",
        "    print(col)\n",
        "    print(df[col].value_counts())\n",
        "    print('='*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKgOopbfuaoC"
      },
      "outputs": [],
      "source": [
        "# How to Encode ?\n",
        "# Example Ever married ‚úÖ\n",
        "'''\n",
        "you can use this to get the values in the feature\n",
        "df['ever_married'].value_counts().index\n",
        "Index(['Yes', 'No'], dtype='object')\n",
        "you can use it in loops for example to fast the mapping and encoding process\n",
        "'''\n",
        "ever_married_mapping = {'Yes' : 1, 'No' : 0}\n",
        "df['ever_married'] = df['ever_married'].map(ever_married_mapping)\n",
        "df[['ever_married']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZPoEo2fuaoD"
      },
      "outputs": [],
      "source": [
        "df['ever_married'].value_counts()\n",
        "# we did here a lable Encodeing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQuenTaRuaoD"
      },
      "outputs": [],
      "source": [
        "# example gender using sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encode categorical features using scikit-learn LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "df['gender'] = encoder.fit_transform(df['gender'])\n",
        "df[['gender']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ-PDnlouaoE"
      },
      "outputs": [],
      "source": [
        "df['gender'].value_counts()\n",
        "# note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKGlzuGSuaoF"
      },
      "outputs": [],
      "source": [
        "# one hot Encodeing\n",
        "# you can use OneHotEncoder if you wish\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# or using pandas get_dummies() easier way\n",
        "df = pd.get_dummies(df, columns=['work_type'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty3cYk-RuaoF"
      },
      "outputs": [],
      "source": [
        "# TODO encode the rest of the column with the way you see suitable üéØ\n",
        "df = None\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM1c6tJSuaoH"
      },
      "outputs": [],
      "source": [
        "# check if there is any other categorical or object columns\n",
        "df.select_dtypes(include=['O', 'category']).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLTUs5DnuaoI"
      },
      "source": [
        "## Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVgiGXl-uaoI"
      },
      "source": [
        "### Why we need to scale the data features?\n",
        "1. **Normalization**: **Scaling** data to a **specific range or distribution** can help normalize the features, making them comparable and ***avoiding bias towards variables with larger scales***.\n",
        "2. **Improved Model Performance**: Many machine learning algorithms are sensitive to the scale of the input features. Scaling can help algorithms converge faster and improve their performance. For example, **distance-based algorithms** like k-nearest neighbors (KNN) and clustering algorithms can be influenced by the scale of the features.\n",
        "3. **Gradient Descent Optimization**: Scaling features can help gradient-based optimization algorithms, it can prevent certain features from dominating the learning process due to their larger scales.\n",
        "4. **Regularization**: Some regularization techniques, like Ridge regression and Lasso regression, are sensitive to feature scales. Scaling can ensure that regularization is applied uniformly across features.\n",
        "5. **Visualization**: Scaling data can make it easier to visualize and interpret the data. Plots and graphs are more meaningful when features are on a similar scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjIomP_uaoJ"
      },
      "source": [
        "### Types of scalers?\n",
        "1. **StandardScaler**: This scaler standardizes features by removing the mean and scaling to unit variance. It transforms the data to have zero mean and unit standard deviation. StandardScaler assumes that the data follows a [Gaussian distributionüîî](!https://hossam-ahmed.notion.site/Normal-distribution-2af1ec317d5f4680867d6ff67a1d8330?pvs=4).\n",
        "2. **MinMaxScaler**: This scaler scales features to a specified range, typically between 0 and 1. It linearly transforms the data, preserving the shape of the original distribution. MinMaxScaler is sensitive to outliers and can compress the data if the range is not carefully set.\n",
        "3. **MaxAbsScaler**: This scaler scales features to the range [-1, 1] by dividing through the maximum absolute value in each feature. It does not shift or center the data, preserving the sparsity of the original data.\n",
        "4. **RobustScaler**: This scaler is robust to outliers and scales features using statistics that are robust to outliers. It removes the median and scales the data according to the Interquartile Range (IQR). RobustScaler is suitable for data that contains significant outliers.\n",
        "5. **QuantileTransformer**: This scaler transforms features to follow a specified distribution, typically a Gaussian distribution. It applies a non-linear transformation that maps the data to a uniform distribution and then to the desired distribution. QuantileTransformer is less sensitive to outliers and can distort correlations and distances between points.\n",
        "6. **PowerTransformer**: This scaler applies a power transformation to make the data more Gaussian-like. It supports two types of transformations: the Box-Cox transformation and the Yeo-Johnson transformation. PowerTransformer is useful when the data does not follow a Gaussian distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncer9kW5uaoK"
      },
      "source": [
        "#### TODO üéØ : What is an outlier?\n",
        ">search and wirite your notes and search results in this Markdown cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MleYPllVuaoL"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxurGHtXuaoM"
      },
      "outputs": [],
      "source": [
        "# importing different scalers to be used\n",
        "# TODO: you should try to scale your dataset with each of these types and build the model -> evaluate it and re do this cycle with different sclaers to note which one is the best\n",
        "# hint üí° if you find this a tedious task you can search for how to use pipelines; this is optional but if you did you learn too much and get bouns‚ú®üéØ\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# StandardScaler example\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])\n",
        "\n",
        "# TODO MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaled_data = None\n",
        "\n",
        "# TODO RobustScaler\n",
        "robust_scaler = RobustScaler()\n",
        "robust_scaled_data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX8sinaCuaoM"
      },
      "outputs": [],
      "source": [
        "# display(scaled_data, min_max_scaled_data, robust_scaled_data)\n",
        "pd.DataFrame(robust_scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipsm-qWluaoN"
      },
      "outputs": [],
      "source": [
        "# for example I am going to use the StandardScale\n",
        "sdf = pd.DataFrame(scaled_data, columns = ['age', 'avg_glucose_level', 'bmi'])\n",
        "sdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56pzFIziuaoO"
      },
      "outputs": [],
      "source": [
        "bf = df\n",
        "bf.loc[:, ['age', 'avg_glucose_level', 'bmi']] = sdf[['age', 'avg_glucose_level', 'bmi']]\n",
        "bf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Diw1ttuaoO"
      },
      "source": [
        "### Extra point üåü optional\n",
        "> Parallel Coordinates Plot, also known as a Parallel Coordinate Plot or PCP, is a multivariate data visualization technique used to display and explore relationships among multiple numeric variables. It's particularly useful for visualizing high-dimensional datasets and identifying patterns, trends, or outliers within the data.\n",
        "1. **Vertical Axes:** Each numeric variable in your dataset is represented by a vertical axis or a parallel line on the plot. These axes are parallel to each other and equally spaced horizontally.\n",
        "2. **Data Lines:** Data points in your dataset are represented as lines that connect values for each variable along the respective axes. Each line represents an individual data point, and its path along the axes reveals how that data point's values compare across variables.\n",
        "\n",
        "**Interpreting a Parallel Coordinates Plot:**\n",
        "\n",
        "- **Patterns:** Parallel Coordinates Plots are excellent for identifying patterns in your data. Patterns may manifest as lines that follow a specific trajectory or show clustering of data points.\n",
        "- **Outliers:** Outliers are data points that deviate significantly from the norm. In a Parallel Coordinates Plot, they often appear as lines that stand out from the majority of the data.\n",
        "- **Correlations:** By examining how lines move in relation to each other, you can infer correlations or dependencies between variables. Variables that move together or exhibit similar trends suggest a positive correlation, while those moving in opposite directions suggest a negative correlation.\n",
        "\n",
        "**Customization and Tips:**\n",
        "\n",
        "- **Normalization:** Since variables may have different scales or units, it's common to normalize the data before creating a Parallel Coordinates Plot to ensure fair comparison.\n",
        "- **Color Mapping:** You can use color to differentiate between different categories or groups within your dataset. Each category can be represented by a unique color for the lines, making it easier to distinguish patterns.\n",
        "- **Axis Reordering:** You can reorder the axes to highlight specific patterns or emphasize certain variables.\n",
        "- **Interactive Plots:** Some visualization libraries and tools allow for interactive Parallel Coordinates Plots, enabling you to filter, zoom, and explore the data dynamically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZeU5I8IuaoP"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "#TODO Create a parallel coordinates plot\n",
        "fig = px.parallel_coordinates(bf, color='stroke')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXP-PqkAuaoP"
      },
      "source": [
        "# <span style=\"color:skyblue\">Model Building : Logistic Regression</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX87GNMduaoQ"
      },
      "source": [
        "## Train and Test sets\n",
        "1. **Training Set:**¬†The training set is a subset of the original dataset used to train the machine learning model. It contains input data (features) and the corresponding known output or target values. During the training phase, the model learns patterns and relationships in the data to make predictions or classify new, unseen data.\n",
        "2. **Test Set:**¬†The test set is another subset of the original dataset that is <span style='color:red'>not used during model training</span>. It serves as an independent dataset to assess how well the trained model generalizes to new, unseen data. The test set usually contains input data, but the corresponding target values are withheld. The model predictions on the test set can be compared to the actual target values to evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0WfoBOUuaoR"
      },
      "source": [
        "### Why we need that?\n",
        "- **Model Evaluation:** By evaluating the model's performance on the test set, you can estimate ***how well it will perform on unseen data***. This evaluation helps assess the model's ability to generalize and provides insights into its strengths and weaknesses.\n",
        "- **Preventing Overfitting:** Overfitting occurs when a model becomes too specialized in the training data and performs poorly on new data. By using a separate test set, you can evaluate the model's performance on unseen data and detect if it is overfitting. This information can guide you in adjusting the model or its hyperparameters to improve generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZzMWpFRuaoS"
      },
      "outputs": [],
      "source": [
        "# How to achieve this concept in code\n",
        "# step1: 1Ô∏è‚É£\n",
        "# TODO Split the data you have to X features and y target\n",
        "# X is all columns except the target stroke\n",
        "\n",
        "X = None\n",
        "y = bf['stroke']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32S3105QuaoS"
      },
      "outputs": [],
      "source": [
        "# now after you have your data splited to features(predictores) independent features and the target the (predicted) dependent feature\n",
        "# you should split to train, and test\n",
        "# we would use somefunction from sklearn to help us having train and test sets\n",
        "# step 2Ô∏è‚É£\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming you have your data and labels (X and y)\n",
        "# X is your feature data, and y is your target variable\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% train, 40% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Parameters:\n",
        "# - X: The feature data\n",
        "# - y: The target variable\n",
        "# - test_size: The proportion of the dataset to include in the test split (e.g., 0.2 for 20% test)\n",
        "# - random_state: An optional random seed for reproducibility\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXJ86fMDuaoT"
      },
      "outputs": [],
      "source": [
        "# training a logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a logistic regression model\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Once fitted, the model is ready to make predictions or be evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QupyNjdtuaoU"
      },
      "source": [
        "# <span style=\"color:skyblue\">Model Evaluation</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LW3bRwOuaoU"
      },
      "outputs": [],
      "source": [
        "# Predict on a new dataset (e.g., your test data)\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jlYVKt8uaoV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.5f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVYl-qZzuaoW"
      },
      "source": [
        "# <span style=\"color:salmon\">Can We do better?</span>\n",
        "- we can see that we have problem of unblanced target\n",
        "- This thing is a common problem in medical problems\n",
        "- Suggested ways to solve it üåü\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIzVQUUIuaoW"
      },
      "source": [
        "> Addressing the issue of unbalanced target classes in a classification problem is important because it can lead to biased models that perform poorly on the minority class. There are several strategies you can employ to handle class imbalance:\n",
        ">\n",
        "1. **Resampling Techniques**:\n",
        "    - **Oversampling**: Increase the number of instances in the minority class by randomly duplicating samples or generating synthetic samples. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can help create synthetic samples.\n",
        "    - **Undersampling**: Reduce the number of instances in the majority class by randomly removing samples. Be cautious with this method as it may lead to loss of important information.\n",
        "2. **Different Algorithms**:\n",
        "    - Some machine learning algorithms are less sensitive to class imbalance. For example, decision trees and random forests can work well with imbalanced data because they can give more weight to the minority class. (third week)\n",
        "3. **Using Different Evaluation Metrics**:\n",
        "    - Instead of accuracy, use evaluation metrics like precision, recall, F1-score, or the area under the ROC curve (AUC-ROC). These metrics consider the performance of the classifier for each class, providing a more comprehensive view of its effectiveness.\n",
        "4. **Class Weights**:\n",
        "    - Many machine learning libraries allow you to assign different weights to classes. By giving more weight to the minority class, you can make the algorithm pay more attention to it during training. In scikit-learn, you can set the `class_weight` parameter in many classifiers.\n",
        "5. **Anomaly Detection**:\n",
        "    - Treat the minority class as an anomaly detection problem. This involves training the model to identify rare events as anomalies.(fourth week)\n",
        "6. **Ensemble Methods**:\n",
        "    - Use ensemble methods like EasyEnsemble, BalanceCascade, or weighted ensembles. These techniques combine multiple models to improve classification performance on imbalanced datasets. (third week)\n",
        "7. **Cost-sensitive Learning**:\n",
        "    - Introduce a cost matrix that assigns different misclassification costs to different classes. This way, the algorithm considers the imbalance during training.\n",
        "8. **Collect More Data**:\n",
        "    - If possible, gather more data for the minority class. This can help balance the dataset naturally.\n",
        "9. **Threshold Adjustment**:\n",
        "    - Adjust the classification threshold. By default, most classifiers use a threshold of 0.5 for binary classification. You can increase or decrease this threshold based on the specific requirements of your problem to prioritize precision or recall.\n",
        "10. **Ensemble of Different Models**:\n",
        "    - Combine multiple models trained on different subsets of the data or using different techniques to improve classification accuracy.(third week)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpLmSQa0uaoX"
      },
      "outputs": [],
      "source": [
        "# Class Weights example of how to deal with unbalanced target\n",
        "'''\n",
        "using the class_weight parameter. Assign a higher weight to the minority class to make the model pay more attention to it during training.\n",
        "'''\n",
        "lg_model_balanced = LogisticRegression(class_weight=[1,7])\n",
        "\n",
        "# Fit the model to the training data\n",
        "lg_model_balanced.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = lg_model_balanced.predict(X_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loXsS27guaoa"
      },
      "source": [
        "# <span style=\"color:skyblue\">Model Deployment</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBB-aQPkuaob"
      },
      "outputs": [],
      "source": [
        "# TODO save the model\n",
        "# after saving the model you may want to use it within a GUI or a web application for exampl a patient input the answer of the questions which represent the\n",
        "# feature you trained the model on, don't forget to do the preprocessing you did or the scalling beacuse the model you save expect a prossed data in a specific format\n",
        "# this point is bounsüå†  and you can do it after the Deadline if you need more time for searching"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}